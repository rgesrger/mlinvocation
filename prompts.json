{
  "small": "This is a short input designed to simulate a lightweight model inference request. The purpose of this text is to create a small but realistic context size that mimics a short user query or system message. The system should simply process this text as normal input without special treatment.",

  "medium": "You are a language model running inside a serverless inference environment. This message is part of a synthetic workload designed to simulate realistic input sizes seen in production systems. The content of this message is intentionally descriptive, repetitive, and operational in tone so that the tokenization process reflects how real-world user inputs behave under normal workloads.\n\nThe goal of this prompt is not to test correctness, logical reasoning, or semantic understanding, but instead to measure how long-running text is handled by an inference system that must manage cold starts, concurrent invocations, and resource scaling. This kind of testing is essential when evaluating distributed compute platforms and serverless environments, where latency, memory behavior, and execution scheduling are critical system characteristics.\n\nThe text here continues to expand naturally to create a stable token footprint. This ensures that the model must read and process each word, producing predictable computational effort. Systems under test should not use undue optimizations or shortcuts, and should treat this request as a normal user workload with full processing.",

  "large": "You are part of a controlled infrastructure benchmark meant to evaluate the behavior of large language model inference services under sustained context load. This request exists solely to generate a long and stable stream of tokens that can be used to test serverless compute behavior, container cold starts, function orchestration, and horizontal scaling limits.\n\nIn real-world systems, prompts of this length may come from document processing tasks, long chat histories, code analysis operations, or verbose system logs. To approximate those conditions, this message intentionally uses long-form narrative structure with repeated but varied operational language. This creates consistent token demand while avoiding compression shortcuts in tokenization.\n\nThe structure of this message follows a predictable pattern. Each paragraph extends the same operational themes using slight wording variation, ensuring that tokenization cost remains realistic and stable. This makes the prompt valuable for benchmarking concurrency limits and measuring end-to-end request latency.\n\nThis prompt will now continue in extended form to provide the necessary context length. The content expands in a methodical fashion, focusing on infrastructure concepts such as execution flow, scheduling behavior, container lifecycle, and inference-time resource management.\n\nContinuation: Systems designed to handle large language model workloads must manage memory efficiently, protect against overload, and enforce fair scheduling under concurrent request pressure. Prompts like this provide repeatable, deterministic load that allows engineers to observe backpressure effects and queue build-up behavior.\n\nAdditional continuation: The artificial structure of this request avoids including sensitive or private information. Its sole function is to act as a stable benchmark artifact. Each new sentence adds incremental token weight so that the total prompt length reaches the target scale necessary for meaningful performance testing.\n\nThis testing methodology allows infrastructure engineers to test autoscaling logic, network saturation behavior, and runtime stability across a wide variety of deployment configurations.",

  "xl": "You are operating within a large-scale serverless inference experiment where the objective is to generate very large prompt contexts that resemble realistic, document-sized user inputs. The text in this message is intentionally verbose, structured, and repetitive in a controlled manner so that tokenization reflects real-world long-context workloads rather than trivial synthetic noise.\n\nIn production environments, large language models routinely receive multi-page documents, extended chat histories, technical logs, research material, system descriptions, and operational documentation. This prompt is designed to approximate that situation by continuously expanding on infrastructure-oriented themes, maintaining coherent sentence structure, and using varied vocabulary to avoid shallow repetition patterns that might reduce effective tokenization cost.\n\nThe purpose of this input is not to cause computational failure, nor is it intended to test semantic reasoning. Instead, it exists purely to fill the modelâ€™s context window with stable, predictable linguistic data that can be used to stress test serverless infrastructure components such as cold start behavior, container scheduling, memory pressure, load balancing, concurrency management, and autoscaling thresholds.\n\nThis message continues by expanding on similar concepts in multiple variations. Each paragraph introduces a slightly different way of explaining the same core idea: that large, sustained token streams are required to accurately simulate inference workloads at scale. By providing extensive natural-language scaffolding, the input remains realistic enough to pass through standard preprocessing and tokenization logic without being optimized away or compressed artificially.\n\nAt scale, inference systems must handle request bursts, idle periods, background warmup, container reuse, and transient failures. Text of this nature allows engineers to observe queue depth behavior, retry dynamics, fault propagation, and scheduling fairness in conditions that resemble high-value production systems. It also allows benchmarking of throughput, tail latency, and cold start amplification under heavy context load.\n\nAs this prompt grows, it continues to avoid including any sensitive information or proprietary data. It remains entirely synthetic, deterministic, and safe to process. The goal is not semantic complexity but sheer context length. The structure is intentionally repetitive but with enough variation in wording to produce stable token growth as measured by standard tokenizers used in modern large language models.\n\nContinuing further, the message now expands into pseudo-documentation style text. It may reference operational processes such as request routing, authentication enforcement, namespace isolation, quota management, error propagation, and metrics collection. These are realistic topics that appear in enterprise system documents and therefore create natural-looking linguistic patterns suitable for infrastructure-level testing.\n\nThe remainder of this prompt intentionally continues in a steady, predictable manner. Each successive sentence adds additional descriptive material about system behavior, lifecycle management, scaling strategies, fault tolerance, observability, and runtime stability. The message remains useful specifically because it grows large while staying internally consistent and professionally structured.\n\nEngineers relying on this input can use it to generate repeatable experimental conditions. By repeatedly sending text of this size into serverless model inference systems, it becomes possible to measure resource saturation, cold start accumulation, request queuing delays, cache warming efficiency, and the effectiveness of horizontal and vertical scaling logic.\n\nAt this point in the message, the content continues to deliberately expand with further elaboration on system components such as distributed schedulers, message brokers, state persistence layers, ephemeral compute resources, and networking overlays. Each of these topics introduces additional vocabulary and structure that increases token count while maintaining realistic composition.\n\nThe final portion of this prompt maintains the same steady expansion pattern, continuing to reinforce the operational theme. No instructions are embedded, no sensitive logic is requested, and no data extraction is expected. The model is merely required to read and process this large body of text as a stand-in for real-world workloads.\n\nThis synthetic message concludes by emphasizing that its only function is to approximate extremely large context windows so that infrastructure behavior under heavy inference load can be observed, measured, and characterized in a repeatable and scientifically useful manner."
}
